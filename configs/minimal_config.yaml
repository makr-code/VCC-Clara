# Minimal-Konfiguration f端r RTX 3060 12GB (Sicher)
# Sehr konservative Einstellungen f端r erfolgreichen Start

# Modell-Einstellungen
model:
  base_model: "LeoLM/leo-hessianai-7b"
  revision: "88c5ac07006ea8f1b5d10aa4f03f0d624dd27e56"
  trust_remote_code: false
  use_flash_attention: false

# Minimal LoRA Parameter
lora:
  r: 8                    # Sehr kleiner Rank
  alpha: 16               # 2*r
  dropout: 0.1
  target_modules:         # Nur 2 Module
    - "q_proj"
    - "v_proj"

# Minimal Daten-Konfiguration  
data:
  train_file: "data/veritas_processed/batch_processed_1756566575.jsonl"
  validation_split: 0.0   # Keine Validation f端r max. Speicher
  max_length: 256         # Sehr kurze Sequenzen
  preprocessing:
    remove_duplicates: true
    min_length: 10
    max_length: 512

# Minimal Training-Parameter
training:
  output_dir: "models/clara_minimal_outputs"
  num_epochs: 1           # Nur eine Epoche f端r Test
  batch_size: 1           
  eval_batch_size: 1      
  gradient_accumulation_steps: 64  # Sehr hohe Akkumulation
  learning_rate: 1e-4     
  weight_decay: 0.01
  warmup_steps: 50        
  logging_steps: 100       
  save_steps: 2000        
  eval_steps: 2000
  save_total_limit: 1     
  fp16: true              
  seed: 42
  dataloader_num_workers: 0  # Kein Parallelismus
  gradient_checkpointing: true
  
# Evaluation
evaluation:
  metric: "perplexity"

# Hardware
hardware:
  use_cuda: true
  device_map: null
  max_memory_gb: 8        # Sehr konservativ

# Deaktiviert
wandb:
  enabled: false
  
ollama:
  convert: false
