data:
  max_length: 1024
  preprocessing:
    max_length: 4096
    min_length: 10
    remove_duplicates: true
  train_file: data/processed/verwaltung_train.jsonl
  validation_split: 0.1
evaluation:
  early_stopping: true
  metric: perplexity
  patience: 5
hardware:
  dataloader_num_workers: 2
  max_memory_per_gpu: 12GB
  pin_memory: false
  use_gradient_checkpointing: true
lora:
  alpha: 128
  dropout: 0.05
  r: 64
  target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
model:
  base_model: LeoLM/leo-hessianai-7b
ollama:
  convert: true
  model_name: clara-leo-hessianai-7b
  output_path: ollama_models/
qlora:
  compute_dtype: bfloat16
  double_quant: true
  quant_type: nf4
training:
  batch_size: 1
  bf16: true
  eval_batch_size: 1
  eval_steps: 250
  gradient_accumulation_steps: 16
  learning_rate: 2e-4
  logging_steps: 5
  num_epochs: 5
  output_dir: models/clara_leo_hessianai_7b_outputs
  save_steps: 250
  save_total_limit: 3
  seed: 42
  warmup_steps: 200
  weight_decay: 0.01
wandb:
  enabled: false
  entity: your_username
  project: CLARA-leo_hessianai_7b
