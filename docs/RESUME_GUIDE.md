# CLARA Training Resume - Anleitung

## ğŸ”„ **Resume-FunktionalitÃ¤t ist jetzt Standard!**

### ğŸ“‹ **Standardverhalten (Neu)**

Wenn Sie ein Training starten und Checkpoints vorhanden sind, wird **automatisch** vom letzten Checkpoint fortgesetzt:

```bash
# Startet automatisch Resume falls Checkpoints vorhanden
python scripts/clara_train_lora.py --config configs/leo_cuda_config.yaml
```

### ğŸ†• **Neues Training erzwingen**

Wenn Sie explizit ein **neues Training** starten mÃ¶chten (Checkpoints ignorieren):

```bash
# Ignoriert vorhandene Checkpoints und startet neu
python scripts/clara_train_lora.py --config configs/leo_cuda_config.yaml --no-resume
```

### ğŸ¯ **Spezifischen Checkpoint verwenden**

Um von einem bestimmten Checkpoint fortzusetzen:

```bash
# Resume von spezifischem Checkpoint
python scripts/clara_train_lora.py --config configs/leo_cuda_config.yaml --resume models/clara_leo_cuda_outputs/checkpoint-1000
```

### ğŸ› ï¸ **Resume-Utility Commands**

**Checkpoints auflisten:**
```bash
python scripts/resume_training.py list
```

**Checkpoint-Details anzeigen:**
```bash
python scripts/resume_training.py info models/clara_leo_cuda_outputs/checkpoint-1000
```

**Training mit Resume starten:**
```bash
python scripts/resume_training.py resume --config configs/leo_cuda_config.yaml
```

**Training neu starten (ohne Resume):**
```bash
python scripts/resume_training.py resume --config configs/leo_cuda_config.yaml --no-resume
```

**Alte Checkpoints bereinigen:**
```bash
python scripts/resume_training.py cleanup --keep 3
```

### ğŸ“Š **Was wird gespeichert?**

Bei jedem Checkpoint werden gespeichert:
- **Modell-Gewichte** (LoRA Adapter)
- **Optimizer-Zustand** (Adam, etc.)
- **Learning Rate Scheduler**
- **Training-Fortschritt** (Step, Epoch)
- **Zufallszahlengenerator-Status**
- **Loss-History und Metriken**

### âš¡ **Quick-Status zeigt Resume-Info**

```bash
python scripts/quick_status.py
```

Zeigt jetzt auch:
- Anzahl verfÃ¼gbarer Checkpoints
- Resume-VerfÃ¼gbarkeit
- Training-Fortschritt mit ETA

### ğŸ® **Beispiel-Workflow**

1. **Training starten:**
   ```bash
   python scripts/clara_train_lora.py --config configs/minimal_config.yaml
   ```

2. **Training unterbrechen** (Ctrl+C)

3. **Status prÃ¼fen:**
   ```bash
   python scripts/quick_status.py
   # Zeigt: "ğŸ”„ Resume verfÃ¼gbar"
   ```

4. **Training fortsetzen:**
   ```bash
   python scripts/clara_train_lora.py --config configs/minimal_config.yaml
   # Automatisches Resume vom letzten Checkpoint!
   ```

### ğŸ’¡ **Vorteile der neuen Resume-Logik**

- âœ… **Sicher**: Nie versehentlich Fortschritt verlieren
- âœ… **Intuitiv**: Resume ist Standard, wie in modernen Tools erwartet
- âœ… **Flexibel**: Explizite Kontrolle mit --no-resume
- âœ… **Transparent**: Klare Logging-Nachrichten
- âœ… **Robust**: Automatische Checkpoint-Erkennung

### ğŸš¨ **Wichtige Hinweise**

- Checkpoints werden alle `save_steps` erstellt (siehe Config)
- Nur die neuesten `save_total_limit` Checkpoints werden behalten
- Resume funktioniert auch bei KonfigurationsÃ¤nderungen (z.B. Batch-Size)
- Multi-GPU Training unterstÃ¼tzt ebenfalls Resume

Das macht CLARA Training viel benutzerfreundlicher! ğŸ‰
