# CLARA - Cognitive Legal and Administrative Reasoning Assistant

```
## ğŸš€ Schnellstart

### 1. Daten vorbereiten
```bash
python scripts/prepare_data.py --input data/raw/verwaltung_texte.txt --output data/processed/
```

### 2. Optimales Modell wÃ¤hlen (NEU!)
```bash
# Automatische Modell-Empfehlung fÃ¼r Ihre Hardware
python scripts/model_selector.py --vram 16 --language deutsch
```

### 3. LoRA Training starten
```bash
python scripts/clara_train_lora.py --config configs/leo_lora_config.yaml    # Deutsch (empfohlen)
# oder
python scripts/train_qlora.py --config configs/qlora_config.yaml      # FÃ¼r weniger VRAM
``` â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— 
â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—
â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘
â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
 â•šâ•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•
```

CLARA ist ein spezialisiertes LoRA/QLoRA Training-System fÃ¼r Large Language Models (LLMs), optimiert fÃ¼r deutsche Verwaltungs- und Rechtsanwendungen. Als Teil des veritas/covina Ã–kosystems ermÃ¶glicht CLARA das effiziente Fine-Tuning von LLMs mit geringem Speicherverbrauch.

> Migration (2025-09): Das frÃ¼here Standard-Quellverzeichnis `Y:\veritas\data` wurde konsolidiert zu `Y:\data`. Alle Dokumentationen und Skripte verwenden jetzt `Y:\data` als Default. Falls Ihr Datenbestand noch im alten Pfad liegt, kÃ¶nnen Sie entweder `--input "Y:\veritas\data"` explizit angeben oder eine Kopie (nicht verschieben) nach `Y:\data` erstellen. Alternativ setzen Sie die Umgebungsvariable `CLARA_DATA_DIR`.

## ğŸš€ Features

- **LoRA Training**: Effizientes Fine-Tuning mit geringem Speicherverbrauch
- **QLoRA Training**: Quantisiertes Training fÃ¼r noch geringeren Speicherverbrauch
- **Multi-Format Support**: PDF, Word, Markdown, JSON, CSV, TXT
- **Ollama Integration**: Nahtlose Integration mit Ollama fÃ¼r lokales Hosting
- **Verwaltungsfokus**: Speziell angepasst fÃ¼r deutsche Verwaltungs- und Rechtsterminologie
- **GPU-Optimiert**: UnterstÃ¼tzung fÃ¼r CUDA und ROCm
- **Intelligente PDF-Extraktion**: OCR-fÃ¤hige Textextraktion aus BehÃ¶rdendokumenten
- **Batch-Processing**: Parallel-Verarbeitung groÃŸer DokumentenbestÃ¤nde

## ğŸ“‹ Voraussetzungen

- Python 3.8+
- CUDA-fÃ¤hige GPU (empfohlen) oder CPU
- Mindestens 16GB RAM (32GB empfohlen)
- Ollama installiert

## ğŸ› ï¸ Installation

1. **Repository klonen und Setup ausfÃ¼hren:**
```bash
git clone <repository-url>
cd verwLLM
pip install -r requirements.txt
```

2. **Ollama installieren:**
```bash
# Folgen Sie den Anweisungen auf https://ollama.ai/
```

3. **Umgebung konfigurieren:**
```bash
cp .env.example .env
# Bearbeiten Sie .env mit Ihren Einstellungen
```

## ğŸ“Š Projektstruktur

```
verwLLM/
â”œâ”€â”€ src/                    # Hauptquellcode
â”‚   â”œâ”€â”€ training/          # Training-Module
â”‚   â”œâ”€â”€ data/             # Datenverarbeitung
â”‚   â””â”€â”€ utils/            # Hilfsfunktionen
â”œâ”€â”€ data/                  # Trainingsdaten
â”‚   â”œâ”€â”€ raw/              # Rohdaten
â”‚   â”œâ”€â”€ processed/        # Verarbeitete Daten
â”‚   â””â”€â”€ examples/         # Beispieldaten
â”œâ”€â”€ models/               # Gespeicherte Modelle
â”œâ”€â”€ configs/              # Konfigurationsdateien
â”œâ”€â”€ scripts/              # AusfÃ¼hrbare Skripte
â””â”€â”€ notebooks/            # Jupyter Notebooks
```

## ğŸ¯ Integration mit veritas/covina

CLARA fÃ¼gt sich nahtlos in das bestehende Ã–kosystem ein:
- **veritas**: Datenquelle und Wissensmanagement
- **covina**: Workflow-Integration und BenutzeroberflÃ¤che  
- **CLARA**: KI-gestÃ¼tztes Reasoning und TextverstÃ¤ndnis

## ğŸ¯ Schnellstart

### 1. Daten vorbereiten
```bash
python scripts/prepare_data.py --input data/raw/verwaltung_texte.txt --output data/processed/
```

### 2. LoRA Training starten
```bash
python scripts/clara_train_lora.py --config configs/lora_config.yaml
```

### 3. QLoRA Training starten
```bash
python scripts/train_qlora.py --config configs/qlora_config.yaml
```

### 4. Modell in Ollama konvertieren
```bash
python scripts/convert_to_ollama.py --model models/lora_model --output ollama_model
```

## âš™ï¸ Konfiguration

Bearbeiten Sie die Konfigurationsdateien in `configs/`:
- `lora_config.yaml`: LoRA-spezifische Einstellungen fÃ¼r CLARA
- `qlora_config.yaml`: QLoRA-spezifische Einstellungen fÃ¼r CLARA
- `veritas_config.yaml`: Optimiert fÃ¼r groÃŸe veritas-Datenverzeichnisse

### Pfadsteuerung

Sie kÃ¶nnen das Quell-Datenverzeichnis Ã¼ber eine Umgebungsvariable Ã¼berschreiben:
```
PowerShell:
$env:CLARA_DATA_DIR = 'Y:\data'

CMD:
set CLARA_DATA_DIR=Y:\data
```
Alle Skripte, die einen Standard-Eingabepfad nutzen, prÃ¼fen zuerst `CLARA_DATA_DIR`.

## ğŸ“ˆ Monitoring

Das Training kann Ã¼ber Tensorboard Ã¼berwacht werden:
```bash
tensorboard --logdir logs/
```

### ğŸ§ª Runtime Metriken & Prometheus Export

CLARA stellt nun einen leichten Metrik-Exporter bereit:

- Counter, Gauges, Summaries (count/sum/avg) und Histogramme (manuell definierte Buckets)
- Audit Events (separat) fÃ¼r Routing & Serving Entscheidungen (`audit/audit_log.jsonl`)
- FastAPI Endpoint `/metrics` im Server (`clara_serve_vllm.py`) gibt Prometheus kompatiblen Plaintext zurÃ¼ck.

Beispiel Start (Fake-Modus ohne vLLM):
```bash
uvicorn scripts.clara_serve_vllm:app --host 0.0.0.0 --port 8001
```
Abruf der Metriken:
```bash
curl http://localhost:8001/metrics
```
Beispielauszug:
```
# TYPE clara_routing_requests_total counter
clara_routing_requests_total 12
# TYPE clara_routing_decision_seconds_sum gauge
clara_routing_decision_seconds_sum 0.143
clara_routing_decision_seconds_count 12
clara_routing_decision_seconds_avg 0.0119
clara_routing_decision_seconds_hist_bucket{le="0.01"} 5
clara_routing_decision_seconds_hist_bucket{le="0.02"} 11
clara_routing_decision_seconds_hist_bucket{le="0.05"} 12
clara_routing_decision_seconds_hist_sum 0.143
clara_routing_decision_seconds_hist_count 12
```

Prometheus `scrape_config` Beispiel:
```yaml
scrape_configs:
	- job_name: clara
		static_configs:
			- targets: ["clara-host:8001"]
		metrics_path: /metrics
```

Hinweise:
- Histogram Buckets sind aktuell statisch im Code (`observe_histogram`) hinterlegt.
- FÃ¼r Produktion wird empfohlen, einen dedizierten Prometheus Aggregator oder Pushgateway nur bei Bedarf zu nutzen.
- Audit Log Rotation & Datenschutz-Konfiguration folgen in spÃ¤terer Version.

## ğŸ¤ Beitragen

BeitrÃ¤ge sind willkommen! Bitte lesen Sie die [CONTRIBUTING.md](CONTRIBUTING.md) fÃ¼r Details.

## ğŸ“„ Lizenz

Dieses Projekt steht unter der MIT-Lizenz. Siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ†˜ Support

Bei Fragen oder Problemen Ã¶ffnen Sie bitte ein Issue im Repository.
