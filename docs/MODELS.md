# CLARA - Verf√ºgbare Basismodelle und Empfehlungen

## üéØ Empfohlene Modelle f√ºr deutsche Verwaltung/Recht

### **Top-Empfehlungen:**

| Modell | Gr√∂√üe | VRAM | Sprache | Empfehlung | Anwendung |
|--------|-------|------|---------|------------|-----------|
| **LeoLM/leo-hessianai-7b** | 7B | 16GB | üá©üá™ Deutsch | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Beste Wahl f√ºr deutsche Verwaltung** |
| **DiscoResearch/DiscoLM-German-7b-v1** | 7B | 16GB | üá©üá™ Deutsch | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Speziell f√ºr deutsche Texte** |
| **microsoft/DialoGPT-medium** | 355M | 4GB | üá∫üá∏ Englisch | ‚≠ê‚≠ê‚≠ê | **F√ºr Tests und kleine GPUs** |
| **meta-llama/Llama-2-7b-chat-hf** | 7B | 16GB | üá∫üá∏ Englisch | ‚≠ê‚≠ê‚≠ê‚≠ê | **Stabil, gut f√ºr Experimente** |
| **mistralai/Mistral-7B-Instruct-v0.2** | 7B | 16GB | üá∫üá∏ Englisch | ‚≠ê‚≠ê‚≠ê‚≠ê | **Sehr gut f√ºr Instruktionen** |

## üìã Vollst√§ndige Modell-√úbersicht

```python
# CLARA Basismodell-Konfigurationen
CLARA_BASE_MODELS = {
    # Deutsche Modelle (EMPFOHLEN f√ºr Verwaltung)
    "deutsche_modelle": {
        "LeoLM/leo-hessianai-7b": {
            "name": "LeoLM 7B",
            "size": "7B",
            "vram_requirement": "16GB",
            "language": "Deutsch",
            "specialization": "Deutsche Sprache, Reasoning",
            "recommendation": 5,
            "use_case": "Verwaltung, Recht, formelle Texte",
            "lora_config": "configs/leo_lora_config.yaml",
            "qlora_config": "configs/leo_qlora_config.yaml"
        },
        "DiscoResearch/DiscoLM-German-7b-v1": {
            "name": "DiscoLM German 7B",
            "size": "7B", 
            "vram_requirement": "16GB",
            "language": "Deutsch",
            "specialization": "Deutsche Konversation",
            "recommendation": 5,
            "use_case": "Beh√∂rdenkommunikation, FAQ",
            "lora_config": "configs/disco_lora_config.yaml",
            "qlora_config": "configs/disco_qlora_config.yaml"
        },
        "malteos/hermeo-7b": {
            "name": "Hermeo 7B",
            "size": "7B",
            "vram_requirement": "16GB", 
            "language": "Deutsch",
            "specialization": "Deutsche Instruktionsbefolgung",
            "recommendation": 4,
            "use_case": "Strukturierte Verwaltungsaufgaben"
        }
    },
    
    # Englische Modelle (Fallback/Experimente)
    "englische_modelle": {
        "meta-llama/Llama-2-7b-chat-hf": {
            "name": "Llama 2 Chat 7B",
            "size": "7B",
            "vram_requirement": "16GB",
            "language": "Englisch",
            "specialization": "Chat, Instruktionen",
            "recommendation": 4,
            "use_case": "Experimente, internationale Texte",
            "lora_config": "configs/llama_lora_config.yaml"
        },
        "mistralai/Mistral-7B-Instruct-v0.2": {
            "name": "Mistral 7B Instruct",
            "size": "7B",
            "vram_requirement": "16GB",
            "language": "Englisch", 
            "specialization": "Instruktionsbefolgung",
            "recommendation": 4,
            "use_case": "Strukturierte Aufgaben",
            "lora_config": "configs/mistral_lora_config.yaml"
        },
        "microsoft/DialoGPT-medium": {
            "name": "DialoGPT Medium",
            "size": "355M",
            "vram_requirement": "4GB",
            "language": "Englisch",
            "specialization": "Dialog",
            "recommendation": 3,
            "use_case": "Tests, schwache Hardware",
            "lora_config": "configs/lora_config.yaml"  # Standard
        }
    },
    
    # Gro√üe Modelle (f√ºr starke Hardware)
    "grosse_modelle": {
        "LeoLM/leo-hessianai-13b": {
            "name": "LeoLM 13B", 
            "size": "13B",
            "vram_requirement": "26GB",
            "language": "Deutsch",
            "specialization": "Erweiterte deutsche Sprachverarbeitung",
            "recommendation": 5,
            "use_case": "Komplexe Rechtsfragen, A100 Hardware",
            "note": "Nur f√ºr starke Hardware"
        },
        "meta-llama/Llama-2-13b-chat-hf": {
            "name": "Llama 2 Chat 13B",
            "size": "13B", 
            "vram_requirement": "26GB",
            "language": "Englisch",
            "specialization": "Erweiterte Konversation",
            "recommendation": 4,
            "use_case": "Komplexe Aufgaben, A100 Hardware"
        }
    },
    
    # Kleine Modelle (f√ºr schwache Hardware)
    "kleine_modelle": {
        "microsoft/DialoGPT-small": {
            "name": "DialoGPT Small",
            "size": "117M",
            "vram_requirement": "2GB", 
            "language": "Englisch",
            "specialization": "Einfache Dialoge",
            "recommendation": 2,
            "use_case": "Prototyping, sehr schwache Hardware"
        },
        "distilbert-base-german-cased": {
            "name": "DistilBERT German",
            "size": "135M",
            "vram_requirement": "2GB",
            "language": "Deutsch", 
            "specialization": "Textverst√§ndnis",
            "recommendation": 3,
            "use_case": "Klassifikation, Embedding"
        }
    }
}
```

## üèÜ Empfehlungen nach Anwendungsfall

### **F√ºr Y:\veritas\data\ (Ihre Anwendung):**
1. **LeoLM/leo-hessianai-7b** - Deutsche Verwaltungssprache ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
2. **DiscoResearch/DiscoLM-German-7b-v1** - Deutsche Konversation ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

### **F√ºr Tests/Entwicklung:**
1. **microsoft/DialoGPT-medium** - Schnell, wenig VRAM ‚≠ê‚≠ê‚≠ê

### **F√ºr Production (starke Hardware):**
1. **LeoLM/leo-hessianai-13b** - Beste Qualit√§t ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

## üîÑ Modell-Wechsel

**CLARA ist modular - ein Training, mehrere Modelle:**

```bash
# Verschiedene Basismodelle mit gleichen Daten
python scripts/clara_train_lora.py --config configs/leo_lora_config.yaml      # Deutsch
python scripts/clara_train_lora.py --config configs/disco_lora_config.yaml    # Deutsch
python scripts/clara_train_lora.py --config configs/mistral_lora_config.yaml  # Englisch

# Alle Modelle in Ollama verf√ºgbar als:
ollama run clara-leo      # Deutsche Verwaltung
ollama run clara-disco    # Deutsche Konversation  
ollama run clara-mistral  # Englische Instruktionen
```

## üíæ VRAM-Anforderungen

| GPU | VRAM | Empfohlenes Modell | Training-Methode |
|-----|------|-------------------|------------------|
| RTX 3060 | 12GB | DialoGPT-medium | LoRA |
| RTX 3070 | 8GB | LeoLM-7b | QLoRA |
| RTX 3080 | 10GB | LeoLM-7b | QLoRA |
| RTX 3090 | 24GB | LeoLM-7b | LoRA |
| RTX 4090 | 24GB | LeoLM-13b | LoRA |
| A100 | 40GB | LeoLM-13b | LoRA |
